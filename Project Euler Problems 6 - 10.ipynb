{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-29T21:54:03.892265Z",
     "start_time": "2020-03-29T21:51:31.930418Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0 total reward: 17.0 eps: 1.0\n",
      "episode: 100 total reward: 9.0 eps: 0.09950371902099892\n",
      "episode: 200 total reward: 39.0 eps: 0.07053456158585983\n",
      "episode: 300 total reward: 10.0 eps: 0.0576390417704235\n",
      "episode: 400 total reward: 148.0 eps: 0.04993761694389223\n",
      "episode: 500 total reward: 104.0 eps: 0.04467670516087703\n",
      "episode: 600 total reward: 29.0 eps: 0.04079085082240021\n",
      "episode: 700 total reward: 140.0 eps: 0.0377694787300249\n",
      "episode: 800 total reward: 32.0 eps: 0.03533326266687867\n",
      "episode: 900 total reward: 200.0 eps: 0.03331483023263848\n",
      "episode: 1000 total reward: 185.0 eps: 0.0316069770620507\n",
      "episode: 1100 total reward: 200.0 eps: 0.03013743873394561\n",
      "episode: 1200 total reward: 200.0 eps: 0.028855492841238062\n",
      "episode: 1300 total reward: 116.0 eps: 0.027724348650071385\n",
      "episode: 1400 total reward: 200.0 eps: 0.02671658425726324\n",
      "episode: 1500 total reward: 200.0 eps: 0.025811286645983367\n",
      "episode: 1600 total reward: 161.0 eps: 0.02499219116020307\n",
      "episode: 1700 total reward: 200.0 eps: 0.024246432248443597\n",
      "episode: 1800 total reward: 200.0 eps: 0.02356368148131365\n",
      "episode: 1900 total reward: 200.0 eps: 0.02293553851298437\n",
      "episode: 2000 total reward: 117.0 eps: 0.022355091700494795\n",
      "episode: 2100 total reward: 178.0 eps: 0.021816595214404266\n",
      "episode: 2200 total reward: 200.0 eps: 0.021315227815974374\n",
      "episode: 2300 total reward: 200.0 eps: 0.02084690996125416\n",
      "episode: 2400 total reward: 200.0 eps: 0.02040816326530612\n",
      "episode: 2500 total reward: 174.0 eps: 0.01999600119960014\n",
      "episode: 2600 total reward: 200.0 eps: 0.0196078431372549\n",
      "episode: 2700 total reward: 88.0 eps: 0.019241446072101123\n",
      "episode: 2800 total reward: 200.0 eps: 0.018894849871330582\n",
      "episode: 2900 total reward: 184.0 eps: 0.018566333001716968\n",
      "episode: 3000 total reward: 146.0 eps: 0.01825437644092281\n",
      "episode: 3100 total reward: 200.0 eps: 0.017957634043632188\n",
      "episode: 3200 total reward: 200.0 eps: 0.01767490804100673\n",
      "episode: 3300 total reward: 200.0 eps: 0.01740512865461766\n",
      "episode: 3400 total reward: 92.0 eps: 0.017147337032429676\n",
      "episode: 3500 total reward: 189.0 eps: 0.01690067088544646\n",
      "episode: 3600 total reward: 134.0 eps: 0.016664352333993333\n",
      "episode: 3700 total reward: 200.0 eps: 0.016437677572823703\n",
      "episode: 3800 total reward: 200.0 eps: 0.01622000804188198\n",
      "episode: 3900 total reward: 200.0 eps: 0.01601076285016887\n",
      "episode: 4000 total reward: 200.0 eps: 0.015809412247806517\n",
      "episode: 4100 total reward: 200.0 eps: 0.015615471979112765\n",
      "episode: 4200 total reward: 200.0 eps: 0.015428498379527544\n",
      "episode: 4300 total reward: 200.0 eps: 0.015248084103296531\n",
      "episode: 4400 total reward: 200.0 eps: 0.015073854388204487\n",
      "episode: 4500 total reward: 200.0 eps: 0.014905463779355262\n",
      "episode: 4600 total reward: 200.0 eps: 0.014742593246782542\n",
      "episode: 4700 total reward: 200.0 eps: 0.014584947642137372\n",
      "episode: 4800 total reward: 200.0 eps: 0.014432253448298278\n",
      "episode: 4900 total reward: 200.0 eps: 0.014284256782850143\n",
      "episode: 5000 total reward: 109.0 eps: 0.014140721622265264\n",
      "episode: 5100 total reward: 182.0 eps: 0.014001428218521149\n",
      "episode: 5200 total reward: 200.0 eps: 0.013866171683985996\n",
      "episode: 5300 total reward: 200.0 eps: 0.013734760723838819\n",
      "episode: 5400 total reward: 190.0 eps: 0.013607016498184134\n",
      "episode: 5500 total reward: 123.0 eps: 0.013482771598464758\n",
      "episode: 5600 total reward: 200.0 eps: 0.01336186912484746\n",
      "episode: 5700 total reward: 200.0 eps: 0.013244161853017133\n",
      "episode: 5800 total reward: 144.0 eps: 0.013129511480316922\n",
      "episode: 5900 total reward: 200.0 eps: 0.013017787942456283\n",
      "episode: 6000 total reward: 119.0 eps: 0.012908868793110689\n",
      "episode: 6100 total reward: 192.0 eps: 0.012802638639684321\n",
      "episode: 6200 total reward: 200.0 eps: 0.012698988629324323\n",
      "episode: 6300 total reward: 72.0 eps: 0.012597815979981676\n",
      "episode: 6400 total reward: 200.0 eps: 0.01249902355192602\n",
      "episode: 6500 total reward: 188.0 eps: 0.01240251945565374\n",
      "episode: 6600 total reward: 200.0 eps: 0.01230821669259179\n",
      "episode: 6700 total reward: 200.0 eps: 0.012216032825403858\n",
      "episode: 6800 total reward: 78.0 eps: 0.01212588967505907\n",
      "episode: 6900 total reward: 126.0 eps: 0.0120377130421331\n",
      "episode: 7000 total reward: 93.0 eps: 0.011951432450083671\n",
      "episode: 7100 total reward: 200.0 eps: 0.011866980908481739\n",
      "episode: 7200 total reward: 200.0 eps: 0.01178429469439066\n",
      "episode: 7300 total reward: 200.0 eps: 0.011703313150272007\n",
      "episode: 7400 total reward: 149.0 eps: 0.011623978496961555\n",
      "episode: 7500 total reward: 200.0 eps: 0.01154623566040508\n",
      "episode: 7600 total reward: 200.0 eps: 0.011470032110973345\n",
      "episode: 7700 total reward: 200.0 eps: 0.011395317714291038\n",
      "episode: 7800 total reward: 200.0 eps: 0.011322044592617122\n",
      "episode: 7900 total reward: 200.0 eps: 0.011250166995905777\n",
      "episode: 8000 total reward: 200.0 eps: 0.01117964118175896\n",
      "episode: 8100 total reward: 200.0 eps: 0.011110425303554916\n",
      "episode: 8200 total reward: 200.0 eps: 0.0110424793061026\n",
      "episode: 8300 total reward: 200.0 eps: 0.010975764828230913\n",
      "episode: 8400 total reward: 200.0 eps: 0.010910245111774533\n",
      "episode: 8500 total reward: 184.0 eps: 0.01084588491646583\n",
      "episode: 8600 total reward: 200.0 eps: 0.010782650440285157\n",
      "episode: 8700 total reward: 200.0 eps: 0.010720509244860595\n",
      "episode: 8800 total reward: 185.0 eps: 0.010659430185543134\n",
      "episode: 8900 total reward: 200.0 eps: 0.01059938334581489\n",
      "episode: 9000 total reward: 123.0 eps: 0.010540339975716555\n",
      "episode: 9100 total reward: 200.0 eps: 0.010482272434006255\n",
      "episode: 9200 total reward: 168.0 eps: 0.010425154133785426\n",
      "episode: 9300 total reward: 200.0 eps: 0.01036895949134883\n",
      "episode: 9400 total reward: 200.0 eps: 0.010313663878035112\n",
      "episode: 9500 total reward: 200.0 eps: 0.010259243574872122\n",
      "episode: 9600 total reward: 200.0 eps: 0.010205675729827259\n",
      "episode: 9700 total reward: 200.0 eps: 0.010152938317487875\n",
      "episode: 9800 total reward: 200.0 eps: 0.010101010101010102\n",
      "episode: 9900 total reward: 200.0 eps: 0.010049870596186849\n",
      "avg reward for last 100 episodes: 186.31\n",
      "total steps: 1696267.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# https://deeplearningcourses.com/c/deep-reinforcement-learning-in-python\n",
    "# https://www.udemy.com/deep-reinforcement-learning-in-python\n",
    "from __future__ import print_function, division\n",
    "from builtins import range\n",
    "# Note: you may need to update your version of future\n",
    "#       if builtins is not defined\n",
    "# sudo pip install -U future\n",
    "\n",
    "import gym\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from gym import wrappers\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "# turns list of integers into an int\n",
    "# Ex.\n",
    "# build_state([1,2,3,4,5]) -> 12345\n",
    "def build_state(features):\n",
    "  return int(\"\".join(map(lambda feature: str(int(feature)), features)))\n",
    "\n",
    "def to_bin(value, bins):\n",
    "  return np.digitize(x=[value], bins=bins)[0]\n",
    "\n",
    "\n",
    "class FeatureTransformer:\n",
    "  def __init__(self):\n",
    "    # Note: to make this better you could look at how often each bin was\n",
    "    # actually used while running the script.\n",
    "    # It's not clear from the high/low values nor sample() what values\n",
    "    # we really expect to get.\n",
    "    self.cart_position_bins = np.linspace(-2.4, 2.4, 9)\n",
    "    self.cart_velocity_bins = np.linspace(-2, 2, 9) # (-inf, inf) (I did not check that these were good values)\n",
    "    self.pole_angle_bins = np.linspace(-0.4, 0.4, 9)\n",
    "    self.pole_velocity_bins = np.linspace(-3.5, 3.5, 9) # (-inf, inf) (I did not check that these were good values)\n",
    "\n",
    "  def transform(self, observation):\n",
    "    # returns an int\n",
    "    cart_pos, cart_vel, pole_angle, pole_vel = observation\n",
    "    return build_state([\n",
    "      to_bin(cart_pos, self.cart_position_bins),\n",
    "      to_bin(cart_vel, self.cart_velocity_bins),\n",
    "      to_bin(pole_angle, self.pole_angle_bins),\n",
    "      to_bin(pole_vel, self.pole_velocity_bins),\n",
    "    ])\n",
    "\n",
    "\n",
    "class Model:\n",
    "  def __init__(self, env, feature_transformer):\n",
    "    self.env = env\n",
    "    self.feature_transformer = feature_transformer\n",
    "\n",
    "    num_states = 10**env.observation_space.shape[0]\n",
    "    num_actions = env.action_space.n\n",
    "    self.Q = np.random.uniform(low=-1, high=1, size=(num_states, num_actions))\n",
    "\n",
    "  def predict(self, s):\n",
    "    x = self.feature_transformer.transform(s)\n",
    "    return self.Q[x]\n",
    "\n",
    "  def update(self, s, a, G):\n",
    "    x = self.feature_transformer.transform(s)\n",
    "    self.Q[x,a] += 1e-2*(G - self.Q[x,a])\n",
    "\n",
    "  def sample_action(self, s, eps):\n",
    "    if np.random.random() < eps:\n",
    "      return self.env.action_space.sample()\n",
    "    else:\n",
    "      p = self.predict(s)\n",
    "      return np.argmax(p)\n",
    "\n",
    "\n",
    "def play_one(model, eps, gamma):\n",
    "  observation = env.reset()\n",
    "  done = False\n",
    "  totalreward = 0\n",
    "  iters = 0\n",
    "  while not done and iters < 10000:\n",
    "    action = model.sample_action(observation, eps)\n",
    "    prev_observation = observation\n",
    "    observation, reward, done, info = env.step(action)\n",
    "\n",
    "    totalreward += reward\n",
    "\n",
    "    if done and iters < 199:\n",
    "      reward = -300\n",
    "\n",
    "    # update the model\n",
    "    G = reward + gamma*np.max(model.predict(observation))\n",
    "    model.update(prev_observation, action, G)\n",
    "\n",
    "    iters += 1\n",
    "\n",
    "  return totalreward\n",
    "\n",
    "\n",
    "def plot_running_avg(totalrewards):\n",
    "  N = len(totalrewards)\n",
    "  running_avg = np.empty(N)\n",
    "  for t in range(N):\n",
    "    running_avg[t] = totalrewards[max(0, t-100):(t+1)].mean()\n",
    "  plt.plot(running_avg)\n",
    "  plt.title(\"Running Average\")\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  env = gym.make('CartPole-v0')\n",
    "  ft = FeatureTransformer()\n",
    "  model = Model(env, ft)\n",
    "  gamma = 0.9\n",
    "\n",
    "  if 'monitor' in sys.argv:\n",
    "    filename = os.path.basename(__file__).split('.')[0]\n",
    "    monitor_dir = './' + filename + '_' + str(datetime.now())\n",
    "    env = wrappers.Monitor(env, monitor_dir)\n",
    "\n",
    "  N = 10000\n",
    "  totalrewards = np.empty(N)\n",
    "  for n in range(N):\n",
    "    eps = 1.0/np.sqrt(n+1)\n",
    "    totalreward = play_one(model, eps, gamma)\n",
    "    totalrewards[n] = totalreward\n",
    "    if n % 100 == 0:\n",
    "      print(\"episode:\", n, \"total reward:\", totalreward, \"eps:\", eps)\n",
    "  print(\"avg reward for last 100 episodes:\", totalrewards[-100:].mean())\n",
    "  print(\"total steps:\", totalrewards.sum())\n",
    "\n",
    "  plt.plot(totalrewards)\n",
    "  plt.title(\"Rewards\")\n",
    "  plt.show()\n",
    "\n",
    "  plot_running_avg(totalrewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
